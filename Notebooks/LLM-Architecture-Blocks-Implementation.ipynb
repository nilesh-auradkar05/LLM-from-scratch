{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12413689,"sourceType":"datasetVersion","datasetId":7829030},{"sourceId":12435155,"sourceType":"datasetVersion","datasetId":7843854}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install uv\n!uv venv gpt2-clone\n!source /kaggle/working/gpt2-clone/bin/activate\n!uv pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu128\n!uv pip install -q huggingface tiktoken","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T19:00:10.229835Z","iopub.execute_input":"2025-07-14T19:00:10.230395Z","iopub.status.idle":"2025-07-14T19:01:03.034633Z","shell.execute_reply.started":"2025-07-14T19:00:10.230363Z","shell.execute_reply":"2025-07-14T19:01:03.033440Z"}},"outputs":[{"name":"stdout","text":"Collecting uv\n  Downloading uv-0.7.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading uv-0.7.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.6/18.6 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: uv\nSuccessfully installed uv-0.7.21\n\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\u001b[0m\nUsing CPython 3.11.13 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\nCreating virtual environment at: \u001b[36mgpt2-clone\u001b[39m\nActivate with: \u001b[32msource gpt2-clone/bin/activate\u001b[39m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-14T19:01:03.036390Z","iopub.execute_input":"2025-07-14T19:01:03.036753Z","iopub.status.idle":"2025-07-14T19:01:03.415790Z","shell.execute_reply.started":"2025-07-14T19:01:03.036714Z","shell.execute_reply":"2025-07-14T19:01:03.415020Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/tokenizer-test-string/regex_test_string.txt\n/kaggle/input/shakesphere-book/shakesphere_book.txt\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Transformer Block From Scratch","metadata":{}},{"cell_type":"code","source":"GPT2_CONFIG_124M = {\n    \"vocab_size\": 50257, # 256 unicode characters + 1 special tokens\n    \"context_length\": 1024,\n    \"emb_dim\": 768,\n    \"num_heads\": 12,\n    \"num_layers\": 12,\n    \"dropout\": 0.1,\n    \"qkv_bias\": False,\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:23.674233Z","iopub.execute_input":"2025-07-14T03:02:23.674948Z","iopub.status.idle":"2025-07-14T03:02:25.950827Z","shell.execute_reply.started":"2025-07-14T03:02:23.674915Z","shell.execute_reply":"2025-07-14T03:02:25.949564Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:25.953333Z","iopub.execute_input":"2025-07-14T03:02:25.953706Z","iopub.status.idle":"2025-07-14T03:02:33.337820Z","shell.execute_reply.started":"2025-07-14T03:02:25.953675Z","shell.execute_reply":"2025-07-14T03:02:33.336159Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Dummy GPT2 Model Implementation","metadata":{}},{"cell_type":"code","source":"# Dummy GPT 2 Class\nclass DummyGPT2Model(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n        self.drop_emb = nn.Dropout(cfg[\"dropout\"])\n\n        # Transformer Block\n        self.transformer_blocks = nn.Sequential(\n            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])]\n        )\n\n        # Layer Normalization\n        self.final_norm = DummyLayerNormalization(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(\n            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n        )\n\n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.transformer_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:33.339508Z","iopub.execute_input":"2025-07-14T03:02:33.341744Z","iopub.status.idle":"2025-07-14T03:02:33.354559Z","shell.execute_reply.started":"2025-07-14T03:02:33.341693Z","shell.execute_reply":"2025-07-14T03:02:33.352450Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Dummy Transformer Block Implementation","metadata":{}},{"cell_type":"code","source":"class DummyTransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n\n    def forward(self, x):\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:33.356339Z","iopub.execute_input":"2025-07-14T03:02:33.357212Z","iopub.status.idle":"2025-07-14T03:02:34.161170Z","shell.execute_reply.started":"2025-07-14T03:02:33.357181Z","shell.execute_reply":"2025-07-14T03:02:34.159825Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Dummy Layer Normalization Implementation","metadata":{}},{"cell_type":"code","source":"class DummyLayerNormalization(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:34.162271Z","iopub.execute_input":"2025-07-14T03:02:34.162555Z","iopub.status.idle":"2025-07-14T03:02:34.187250Z","shell.execute_reply.started":"2025-07-14T03:02:34.162534Z","shell.execute_reply":"2025-07-14T03:02:34.185908Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"torch.set_printoptions(sci_mode=False)\ntest_input = torch.randn(2, 5)\nlayer_norm = DummyLayerNormalization(emb_dim=5)\nout_layer_norm = layer_norm(test_input)\nmean = out_layer_norm.mean(dim=-1, keepdim=True)\nvar = out_layer_norm.var(dim=-1, unbiased=False, keepdim=True)\nprint(\"Mean: \", mean)\nprint(\"Var: \", var)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:34.188550Z","iopub.execute_input":"2025-07-14T03:02:34.188986Z","iopub.status.idle":"2025-07-14T03:02:34.240054Z","shell.execute_reply.started":"2025-07-14T03:02:34.188949Z","shell.execute_reply":"2025-07-14T03:02:34.238575Z"}},"outputs":[{"name":"stdout","text":"Mean:  tensor([[    0.0000],\n        [    0.0000]], grad_fn=<MeanBackward1>)\nVar:  tensor([[1.0000],\n        [1.0000]], grad_fn=<VarBackward0>)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## GELU Activation Function Implementation","metadata":{}},{"cell_type":"code","source":"class GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n                        (x * 0.044715 * torch.pow(x, 3))))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:34.241507Z","iopub.execute_input":"2025-07-14T03:02:34.242135Z","iopub.status.idle":"2025-07-14T03:02:34.252751Z","shell.execute_reply.started":"2025-07-14T03:02:34.242064Z","shell.execute_reply":"2025-07-14T03:02:34.250721Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Feed Forward Neural Network Block Implementation for Transformer Block","metadata":{}},{"cell_type":"code","source":"class FeedForwardBlock(nn.Module):\n    def __init__(self,cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n            GELU(),\n            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n        )\n\n    def forward(self, x):\n        return self.layers(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:34.256512Z","iopub.execute_input":"2025-07-14T03:02:34.256983Z","iopub.status.idle":"2025-07-14T03:02:34.284774Z","shell.execute_reply.started":"2025-07-14T03:02:34.256955Z","shell.execute_reply":"2025-07-14T03:02:34.283187Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"ffn_block = FeedForwardBlock(GPT2_CONFIG_124M)\nffn_test_input = torch.rand(2, 3, 768)\nffn_output = ffn_block(ffn_test_input)\nprint(ffn_output)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:34.286044Z","iopub.execute_input":"2025-07-14T03:02:34.286449Z","iopub.status.idle":"2025-07-14T03:02:34.425965Z","shell.execute_reply.started":"2025-07-14T03:02:34.286422Z","shell.execute_reply":"2025-07-14T03:02:34.424569Z"}},"outputs":[{"name":"stdout","text":"tensor([[[-0.0395,  0.0814,  0.0481,  ..., -0.0012,  0.0647,  0.0210],\n         [-0.0755,  0.0986, -0.0302,  ..., -0.1156,  0.0041,  0.1058],\n         [-0.0345,  0.0648, -0.0355,  ..., -0.0567,  0.0021,  0.0215]],\n\n        [[ 0.0659,  0.0464, -0.0219,  ..., -0.0556,  0.0058, -0.0541],\n         [-0.0706, -0.0084, -0.0039,  ..., -0.1212,  0.0525,  0.0296],\n         [-0.0359,  0.0533,  0.0317,  ..., -0.0070,  0.0306,  0.0274]]],\n       grad_fn=<ViewBackward0>)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"## Shortcut Connections or Residual Connections or Skip Connections Implementation","metadata":{}},{"cell_type":"code","source":"class ShortcutConnectionExample(nn.Module):\n    def __init__(self, layer_sizes, use_shortcut):\n        super().__init__()\n        self.use_shortcut = use_shortcut\n        self.layers = nn.Sequential(\n            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),\n        )\n\n    def forward(self, x):\n        for layer in self.layers:\n            layer_output = layer(x)\n\n            if self.use_shortcut and x.shape == layer_output.shape:\n                x = x + layer_output\n            else:\n                x = layer_output\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:34.427298Z","iopub.execute_input":"2025-07-14T03:02:34.427687Z","iopub.status.idle":"2025-07-14T03:02:34.439786Z","shell.execute_reply.started":"2025-07-14T03:02:34.427653Z","shell.execute_reply":"2025-07-14T03:02:34.438145Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"layer_sizes = [3, 3, 3, 3, 3, 1]\nsample_input = torch.tensor([[1., 0., 1.]])\ntorch.manual_seed(47)\nmodel_without_shortcut = ShortcutConnectionExample(\n    layer_sizes, use_shortcut=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:34.440725Z","iopub.execute_input":"2025-07-14T03:02:34.441059Z","iopub.status.idle":"2025-07-14T03:02:34.472725Z","shell.execute_reply.started":"2025-07-14T03:02:34.441034Z","shell.execute_reply":"2025-07-14T03:02:34.471178Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"model_with_shortcut = ShortcutConnectionExample(\n    layer_sizes, use_shortcut=True,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:34.474066Z","iopub.execute_input":"2025-07-14T03:02:34.474413Z","iopub.status.idle":"2025-07-14T03:02:34.489941Z","shell.execute_reply.started":"2025-07-14T03:02:34.474388Z","shell.execute_reply":"2025-07-14T03:02:34.488315Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def print_gradients(model, x):\n    output = model(x)\n    target = torch.tensor([[0.]])\n\n    loss = nn.MSELoss()\n    loss = loss(output, target)\n\n    loss.backward()\n\n    for name, param in model.named_parameters():\n        if \"weight\" in name:\n            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:34.491032Z","iopub.execute_input":"2025-07-14T03:02:34.491765Z","iopub.status.idle":"2025-07-14T03:02:34.519883Z","shell.execute_reply.started":"2025-07-14T03:02:34.491722Z","shell.execute_reply":"2025-07-14T03:02:34.518366Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"print(\"Gradients without Shortcut Connections:\")\nprint_gradients(model_without_shortcut, sample_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:34.521378Z","iopub.execute_input":"2025-07-14T03:02:34.521787Z","iopub.status.idle":"2025-07-14T03:02:34.567413Z","shell.execute_reply.started":"2025-07-14T03:02:34.521756Z","shell.execute_reply":"2025-07-14T03:02:34.566190Z"}},"outputs":[{"name":"stdout","text":"Gradients without Shortcut Connections:\nlayers.0.0.weight has gradient mean of 0.00021615072910208255\nlayers.1.0.weight has gradient mean of 0.0003711592289619148\nlayers.2.0.weight has gradient mean of 0.0008400255464948714\nlayers.3.0.weight has gradient mean of 0.006596562918275595\nlayers.4.0.weight has gradient mean of 0.02242942713201046\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"print(\"Gradients with Shortcut Connections:\")\nprint_gradients(model_with_shortcut, sample_input)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T03:02:34.568677Z","iopub.execute_input":"2025-07-14T03:02:34.569071Z","iopub.status.idle":"2025-07-14T03:02:34.585431Z","shell.execute_reply.started":"2025-07-14T03:02:34.569036Z","shell.execute_reply":"2025-07-14T03:02:34.583895Z"}},"outputs":[{"name":"stdout","text":"Gradients with Shortcut Connections:\nlayers.0.0.weight has gradient mean of 0.2044905722141266\nlayers.1.0.weight has gradient mean of 0.30301108956336975\nlayers.2.0.weight has gradient mean of 0.2699629068374634\nlayers.3.0.weight has gradient mean of 0.289590984582901\nlayers.4.0.weight has gradient mean of 1.3344513177871704\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":" # Complete GPT-2 Architecture Implementation","metadata":{}},{"cell_type":"code","source":"GPT_2_CONFIG_124M = {\n    \"vocab_size\": 50257,\n    \"context_length\": 1024,\n    \"embedding_dim\": 768,\n    \"num_heads\": 12,\n    \"num_layers\": 12,\n    \"drop_rate\": 0.1,\n    \"qkv_bias\": False\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T18:46:58.603856Z","iopub.execute_input":"2025-07-14T18:46:58.606182Z","iopub.status.idle":"2025-07-14T18:46:58.612218Z","shell.execute_reply.started":"2025-07-14T18:46:58.606136Z","shell.execute_reply":"2025-07-14T18:46:58.611043Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T19:04:56.996284Z","iopub.execute_input":"2025-07-14T19:04:56.998162Z","iopub.status.idle":"2025-07-14T19:04:57.004226Z","shell.execute_reply.started":"2025-07-14T19:04:56.998113Z","shell.execute_reply":"2025-07-14T19:04:57.003213Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class LayerNormalization(nn.Module):\n    def __init__(self, embedding_dim):\n        super().__init__()\n        self.eps = 1e-5\n        # Trainable parameters\n        self.scale = nn.Parameter(torch.ones(embedding_dim))\n        self.shift = nn.Parameter(torch.zeros(embedding_dim))\n\n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n        return norm_x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T19:13:52.363557Z","iopub.execute_input":"2025-07-14T19:13:52.365063Z","iopub.status.idle":"2025-07-14T19:13:52.372077Z","shell.execute_reply.started":"2025-07-14T19:13:52.365030Z","shell.execute_reply":"2025-07-14T19:13:52.371172Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class FeedForwardBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg[\"embedding_dim\"], 4*cfg[\"embedding_dim\"]),\n            F.gelu(approximate=\"tanh\"),\n            nn.Linear(4*cfg[\"embedding_dim\"], cfg[\"embedding_dim\"])\n        )\n    def forward(self, x):\n        return self.layers(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-14T19:20:35.100635Z","iopub.execute_input":"2025-07-14T19:20:35.101579Z","iopub.status.idle":"2025-07-14T19:20:35.107078Z","shell.execute_reply.started":"2025-07-14T19:20:35.101544Z","shell.execute_reply":"2025-07-14T19:20:35.106246Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, cfg)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}