{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:31:09.339460Z","iopub.status.busy":"2025-07-17T19:31:09.338803Z","iopub.status.idle":"2025-07-17T19:31:15.637691Z","shell.execute_reply":"2025-07-17T19:31:15.636770Z","shell.execute_reply.started":"2025-07-17T19:31:09.339429Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting uv\n","  Downloading uv-0.7.22-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n","Downloading uv-0.7.22-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.7/18.7 MB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: uv\n","Successfully installed uv-0.7.22\n","\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\u001b[0m\n","Using CPython 3.11.13 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\n","Creating virtual environment at: \u001b[36mgpt2-clone\u001b[39m\n","Activate with: \u001b[32msource gpt2-clone/bin/activate\u001b[39m\n"]}],"source":["!pip install uv\n","!uv venv gpt2-clone\n","!source /kaggle/working/gpt2-clone/bin/activate"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:31:15.639797Z","iopub.status.busy":"2025-07-17T19:31:15.639503Z","iopub.status.idle":"2025-07-17T19:32:02.187968Z","shell.execute_reply":"2025-07-17T19:32:02.186921Z","shell.execute_reply.started":"2025-07-17T19:31:15.639772Z"},"trusted":true},"outputs":[],"source":["!uv pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","!uv pip install -q huggingface tiktoken datasets transformers tqdm"]},{"cell_type":"code","execution_count":3,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-07-17T19:32:02.189367Z","iopub.status.busy":"2025-07-17T19:32:02.189108Z","iopub.status.idle":"2025-07-17T19:32:02.470651Z","shell.execute_reply":"2025-07-17T19:32:02.470017Z","shell.execute_reply.started":"2025-07-17T19:32:02.189341Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/shakesphere-book/shakesphere_book.txt\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["## Imports"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:32:02.472565Z","iopub.status.busy":"2025-07-17T19:32:02.472164Z","iopub.status.idle":"2025-07-17T19:32:08.681004Z","shell.execute_reply":"2025-07-17T19:32:08.680378Z","shell.execute_reply.started":"2025-07-17T19:32:02.472545Z"},"trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from datasets import load_dataset\n","from transformers import get_cosine_schedule_with_warmup\n","from tqdm.auto import tqdm"]},{"cell_type":"markdown","metadata":{},"source":["# Dataset Preparation"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:32:08.682014Z","iopub.status.busy":"2025-07-17T19:32:08.681684Z","iopub.status.idle":"2025-07-17T19:48:46.356992Z","shell.execute_reply":"2025-07-17T19:48:46.356379Z","shell.execute_reply.started":"2025-07-17T19:32:08.681997Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8c8b186e28464155b2b3631a2d63a11b","version_major":2,"version_minor":0},"text/plain":["README.md: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"db3dbedbcef34d0cb0ee896443dc74a2","version_major":2,"version_minor":0},"text/plain":["bookcorpus.py: 0.00B [00:00, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c1e623dc40634e47b81ac62c43932bfe","version_major":2,"version_minor":0},"text/plain":["Downloading data:   0%|          | 0.00/1.18G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a66bc3b26254f669c658635ac2f50ca","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/74004228 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["full_book_corpus = load_dataset(\"bookcorpus\", trust_remote_code=True)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:48:46.358109Z","iopub.status.busy":"2025-07-17T19:48:46.357888Z","iopub.status.idle":"2025-07-17T19:48:46.363104Z","shell.execute_reply":"2025-07-17T19:48:46.362545Z","shell.execute_reply.started":"2025-07-17T19:48:46.358092Z"},"trusted":true},"outputs":[],"source":["# Downloading Dataset\n","def load_bookcorpus():\n","    if \"train\" in full_book_corpus:\n","        full_dataset = full_book_corpus[\"train\"]\n","        total_dataset_size = len(full_dataset)\n","        print(f\"Total Dataset Size: {full_dataset}\")\n","        print(\"Extracting subset of 500,000\")\n","        book_corpus = full_dataset.select(range(1_000_000))\n","        print(f\"Final Dataset Size after Cropping: {len(book_corpus)}\")\n","        first_example = book_corpus[0]\n","        key = list(first_example.keys())[0]\n","        for i in range(10):\n","            print(book_corpus[i][key])\n","        return book_corpus\n","    else:\n","        print(\"The dataset has no training split.\")\n","        book_corpus = full_book_corpus.select(range(1_000_000))\n","        return book_corpus"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:48:46.364904Z","iopub.status.busy":"2025-07-17T19:48:46.364513Z","iopub.status.idle":"2025-07-17T19:48:46.391647Z","shell.execute_reply":"2025-07-17T19:48:46.390714Z","shell.execute_reply.started":"2025-07-17T19:48:46.364883Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Dataset Size: Dataset({\n","    features: ['text'],\n","    num_rows: 74004228\n","})\n","Extracting subset of 500,000\n","Final Dataset Size after Cropping: 1000000\n","usually , he would be tearing around the living room , playing with his toys .\n","but just one look at a minion sent him practically catatonic .\n","that had been megan 's plan when she got him dressed earlier .\n","he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins , along with her brothers , mason was often exposed to things that were older .\n","she liked to think being surrounded by adults and older kids was one reason why he was a such a good talker for his age .\n","`` are n't you being a good boy ? ''\n","she said .\n","mason barely acknowledged her .\n","instead , his baby blues remained focused on the television .\n","since the movie was almost over , megan knew she better slip into the bedroom and finish getting ready .\n"]}],"source":["torch.manual_seed(47)\n","book_corpus = load_bookcorpus()"]},{"cell_type":"markdown","metadata":{},"source":["# GPT-2 Architecture Implementation"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:38.359038Z","iopub.status.busy":"2025-07-17T19:49:38.358305Z","iopub.status.idle":"2025-07-17T19:49:38.362594Z","shell.execute_reply":"2025-07-17T19:49:38.361922Z","shell.execute_reply.started":"2025-07-17T19:49:38.359011Z"},"trusted":true},"outputs":[],"source":["GPT2_CONFIG_124M = {\n","    \"vocab_size\": 50257,\n","    \"context_length\": 512,\n","    \"embedding_dim\": 768,\n","    \"num_heads\": 12,\n","    \"num_layers\": 12,\n","    \"drop_rate\": 0.1,\n","    \"qkv_bias\": False,\n","}"]},{"cell_type":"markdown","metadata":{},"source":["### Layer Normalization Block"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:41.223878Z","iopub.status.busy":"2025-07-17T19:49:41.223311Z","iopub.status.idle":"2025-07-17T19:49:41.228736Z","shell.execute_reply":"2025-07-17T19:49:41.227900Z","shell.execute_reply.started":"2025-07-17T19:49:41.223854Z"},"trusted":true},"outputs":[],"source":["class LayerNormalization(nn.Module):\n","    def __init__(self, embedding_dim):\n","        super().__init__()\n","        self.eps = 1e-5\n","        self.scale = nn.Parameter(torch.ones(embedding_dim))\n","        self.shift = nn.Parameter(torch.zeros(embedding_dim))\n","\n","    def forward(self, x):\n","        mean = x.mean(dim=-1, keepdim=True)\n","        var = x.var(dim=-1, keepdim=True, unbiased=False)\n","        norm_x = (x - mean) / (torch.sqrt(var + self.eps))\n","        return self.scale * norm_x + self.shift"]},{"cell_type":"markdown","metadata":{},"source":["### Feed-Forward Block"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:41.900131Z","iopub.status.busy":"2025-07-17T19:49:41.899823Z","iopub.status.idle":"2025-07-17T19:49:41.904704Z","shell.execute_reply":"2025-07-17T19:49:41.904039Z","shell.execute_reply.started":"2025-07-17T19:49:41.900108Z"},"trusted":true},"outputs":[],"source":["# GELU implementation\n","class GELU(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","    def forward(self, x):\n","        return 0.5 * x * (1 + torch.tanh(\n","            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n","            (x + 0.044715 * torch.pow(x, 3))\n","        ))"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:43.887898Z","iopub.status.busy":"2025-07-17T19:49:43.887168Z","iopub.status.idle":"2025-07-17T19:49:43.892176Z","shell.execute_reply":"2025-07-17T19:49:43.891459Z","shell.execute_reply.started":"2025-07-17T19:49:43.887874Z"},"trusted":true},"outputs":[],"source":["class FeedForwardNNBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.layers = nn.Sequential(\n","            nn.Linear(cfg[\"embedding_dim\"], 4 * cfg[\"embedding_dim\"]),\n","            GELU(),\n","            nn.Linear(4 * cfg[\"embedding_dim\"], cfg[\"embedding_dim\"]),\n","        )\n","\n","    def forward(self, x):\n","        return self.layers(x)"]},{"cell_type":"markdown","metadata":{},"source":["### Multi-Head Attention Block"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:44.513921Z","iopub.status.busy":"2025-07-17T19:49:44.513171Z","iopub.status.idle":"2025-07-17T19:49:44.521812Z","shell.execute_reply":"2025-07-17T19:49:44.521010Z","shell.execute_reply.started":"2025-07-17T19:49:44.513895Z"},"trusted":true},"outputs":[],"source":["class MultiHeadAttention(nn.Module):\n","    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n","        super().__init__()\n","        assert (d_out % num_heads == 0), \\\n","            \"d_out should be divisble by num_heads\"\n","        \n","        self.d_out = d_out\n","        self.num_heads = num_heads\n","        self.head_dim = d_out // num_heads\n","\n","        # initializing weight matrices\n","        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n","        self.out_proj = nn.Linear(d_out, d_out)\n","        self.dropout = nn.Dropout(dropout)\n","        self.register_buffer(\n","            \"mask\",\n","            torch.triu(\n","                torch.ones(context_length, context_length),\n","                diagonal=1\n","            )\n","        )\n","\n","    def forward(self, x):\n","        batch_size, num_tokens, d_in = x.shape\n","\n","        # input * weight matrices\n","        queries = self.W_query(x)\n","        keys = self.W_key(x)\n","        values = self.W_value(x)\n","\n","        # Roll out last dim \"d_out\" to num_heads and head_dim\n","        # (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n","        queries = queries.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n","        keys = keys.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n","        values = values.view(batch_size, num_tokens, self.num_heads, self.head_dim)\n","\n","        # Transpose to (b, num_heads, num_tokens, head_dim)\n","        queries = queries.transpose(1, 2)\n","        keys = keys.transpose(1, 2)\n","        values = values.transpose(1, 2)\n","\n","        # Compute Attention scores\n","        attn_scores = queries @ keys.transpose(2, 3)\n","\n","        # mask future tokens\n","        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n","        attn_scores.masked_fill(mask_bool, -torch.inf)\n","\n","        # Compute Attention Weights\n","        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n","        attn_weights = self.dropout(attn_weights)\n","\n","        # Compute Context Vector Matrix\n","        context_vec = (attn_weights @ values).transpose(1, 2)\n","        context_vec = context_vec.contiguous().view(batch_size, num_tokens, self.d_out)\n","        context_vec = self.out_proj(context_vec)\n","\n","        return context_vec"]},{"cell_type":"markdown","metadata":{},"source":["## Transformer Block"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:47.114303Z","iopub.status.busy":"2025-07-17T19:49:47.113982Z","iopub.status.idle":"2025-07-17T19:49:47.119961Z","shell.execute_reply":"2025-07-17T19:49:47.119362Z","shell.execute_reply.started":"2025-07-17T19:49:47.114272Z"},"trusted":true},"outputs":[],"source":["class TransformerBlock(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.mask_attn = MultiHeadAttention(\n","            d_in=cfg[\"embedding_dim\"],\n","            d_out=cfg[\"embedding_dim\"],\n","            context_length=cfg[\"context_length\"],\n","            dropout=cfg[\"drop_rate\"],\n","            num_heads=cfg[\"num_heads\"],\n","            qkv_bias=cfg[\"qkv_bias\"],\n","        )\n","        self.ffn_block = FeedForwardNNBlock(cfg)\n","        self.norm_1 = LayerNormalization(cfg[\"embedding_dim\"])\n","        self.norm_2 = LayerNormalization(cfg[\"embedding_dim\"])\n","        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n","\n","    def forward(self, x):\n","        # Block 1\n","        # residual connection for attention block\n","        shortcut = x\n","        x = self.norm_1(x)\n","        x = self.mask_attn(x)\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut\n","\n","        # Block 2\n","        shortcut = x\n","        x = self.norm_2(x)\n","        x = self.ffn_block(x)\n","        x = self.drop_shortcut(x)\n","        x = x + shortcut\n","\n","        return x"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:49.057078Z","iopub.status.busy":"2025-07-17T19:49:49.056509Z","iopub.status.idle":"2025-07-17T19:49:49.062804Z","shell.execute_reply":"2025-07-17T19:49:49.062147Z","shell.execute_reply.started":"2025-07-17T19:49:49.057056Z"},"trusted":true},"outputs":[],"source":["class GPT2Model(nn.Module):\n","    def __init__(self, cfg):\n","        super().__init__()\n","        self.tok_embeddings = nn.Embedding(cfg[\"vocab_size\"], cfg[\"embedding_dim\"])\n","        self.pos_embeddings = nn.Embedding(cfg[\"context_length\"], cfg[\"embedding_dim\"])\n","        self.drop_embeddings = nn.Dropout(cfg[\"drop_rate\"])\n","\n","        self.transformer_blocks = nn.Sequential(\n","            *[TransformerBlock(cfg) for _ in range(cfg[\"num_layers\"])]\n","        )\n","\n","        self.final_norm = LayerNormalization(cfg[\"embedding_dim\"])\n","        self.out_head = nn.Linear(\n","            cfg[\"embedding_dim\"], cfg[\"vocab_size\"], bias=False\n","        )\n","\n","    def forward(self, in_idx):\n","        batch_size, seq_len = in_idx.shape\n","        tok_embeddings = self.tok_embeddings(in_idx)\n","        pos_embeddings = self.pos_embeddings(torch.arange(seq_len, device=in_idx.device))\n","\n","        x = tok_embeddings + pos_embeddings\n","        x = self.drop_embeddings(x)\n","        x = self.transformer_blocks(x)\n","        x = self.final_norm(x)\n","        logits = self.out_head(x)\n","        return logits"]},{"cell_type":"markdown","metadata":{},"source":["## Model Initialization"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:49.615274Z","iopub.status.busy":"2025-07-17T19:49:49.614966Z","iopub.status.idle":"2025-07-17T19:49:51.040425Z","shell.execute_reply":"2025-07-17T19:49:51.039770Z","shell.execute_reply.started":"2025-07-17T19:49:49.615251Z"},"trusted":true},"outputs":[],"source":["torch.manual_seed(47)\n","model = GPT2Model(GPT2_CONFIG_124M)"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:51.348328Z","iopub.status.busy":"2025-07-17T19:49:51.348019Z","iopub.status.idle":"2025-07-17T19:49:51.355637Z","shell.execute_reply":"2025-07-17T19:49:51.354891Z","shell.execute_reply.started":"2025-07-17T19:49:51.348305Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["total number of parameters in the model: 162,616,320\n"]}],"source":["total_params = sum(p.numel() for p in model.parameters())\n","print(f\"total number of parameters in the model: {total_params:,}\")"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:51.663866Z","iopub.status.busy":"2025-07-17T19:49:51.663377Z","iopub.status.idle":"2025-07-17T19:49:51.667778Z","shell.execute_reply":"2025-07-17T19:49:51.666943Z","shell.execute_reply.started":"2025-07-17T19:49:51.663842Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Total Architecture trainable parameters without output head weights: 124,018,944\n"]}],"source":["gpt_2_model_params = total_params - sum(p.numel() for p in model.out_head.parameters())\n","print(f\"Total Architecture trainable parameters without output head weights: {gpt_2_model_params:,}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Logits to output tokens."]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:56.864697Z","iopub.status.busy":"2025-07-17T19:49:56.864428Z","iopub.status.idle":"2025-07-17T19:49:56.869789Z","shell.execute_reply":"2025-07-17T19:49:56.868868Z","shell.execute_reply.started":"2025-07-17T19:49:56.864677Z"},"trusted":true},"outputs":[],"source":["def generate_text_v1(model, idx, max_new_tokens, context_size):\n","    \"\"\"Get last row from logits for each bach. fetch token with max value. \n","        Append to input and repeat.\n","        Optional: Convert token id to text and display the generated text.\n","    \"\"\"\n","    for _ in range(max_new_tokens):\n","\n","        # 1. truncate input if larger than context size\n","        idx_cond = idx[:, -context_size:]\n","\n","        # 2. Get the predictions\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","\n","        # 3. Retrive only the last row from each batch\n","        logits = logits[:, -1, :]\n","\n","        # 4. Applying softmax to logits\n","        probas = torch.softmax(logits, dim=-1)\n","\n","        # 5. Get index of the vocab entry with the highest probability\n","        idx_next = torch.argmax(probas, dim=-1, keepdim=True)\n","\n","        # 6. Append Retrived token id to original input\n","        idx = torch.cat((idx, idx_next), dim=1)\n","\n","    return idx"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:49:57.330773Z","iopub.status.busy":"2025-07-17T19:49:57.330500Z","iopub.status.idle":"2025-07-17T19:49:57.335258Z","shell.execute_reply":"2025-07-17T19:49:57.334448Z","shell.execute_reply.started":"2025-07-17T19:49:57.330752Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Token Embedding Shape: torch.Size([50257, 768])\n","Output layer shape: torch.Size([50257, 768])\n"]}],"source":["print(f\"Token Embedding Shape: {model.tok_embeddings.weight.shape}\")\n","print(f\"Output layer shape: {model.out_head.weight.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Creating DataLoaders"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:50:02.984564Z","iopub.status.busy":"2025-07-17T19:50:02.984274Z","iopub.status.idle":"2025-07-17T19:50:03.007516Z","shell.execute_reply":"2025-07-17T19:50:03.006817Z","shell.execute_reply.started":"2025-07-17T19:50:02.984543Z"},"trusted":true},"outputs":[],"source":["# Creating Dataset for training.\n","import tiktoken\n","from torch.utils.data import Dataset, DataLoader\n","\n","class BookCorpusDataset(Dataset):\n","    def __init__(self, book_corpus, tokenizer, context_length, stride):\n","        self.input_ids = []\n","        self.target_ids = []\n","\n","        # Joining iterable dict into a string\n","        print(\"Tokenizing and Chunking data...\")\n","        all_token_ids = []\n","        \n","        for content in tqdm(book_corpus, desc=\"Tokenizing example\"):\n","            text = content['text']\n","            if text:\n","                # Tokenize the entire text\n","                token_ids = tokenizer.encode_ordinary(text)\n","                all_token_ids.extend(token_ids)\n","\n","        print(f\"Total Tokens Processed: {len(all_token_ids)}\")\n","\n","        # sliding window approach to chunk the text as input and output tokens of context_size\n","        for i in tqdm(range(0, len(all_token_ids) - context_length, stride), desc=\"Creating chunks\"):\n","            input_chunk = all_token_ids[i : i+context_length]\n","            target_chunk = all_token_ids[i+1 : i+context_length+1]\n","            self.input_ids.append(torch.tensor(input_chunk))\n","            self.target_ids.append(torch.tensor(target_chunk))\n","\n","    def __len__(self):\n","        return len(self.input_ids)\n","\n","    def __getitem__(self, idx):\n","        return self.input_ids[idx], self.target_ids[idx]\n","\n","def create_dataloader(book_corpus, batch_size=4, context_length=512,\n","                      stride=512, shuffle=True, drop_last=True, num_workers=0):\n","\n","    try:\n","        # initializing tokenizer\n","        tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","        # creating dataset\n","        dataset = BookCorpusDataset(book_corpus, tokenizer, context_length, stride)\n","\n","        # create dataloader\n","        dataloader = DataLoader(\n","            dataset,\n","            batch_size=batch_size,\n","            shuffle=shuffle,\n","            drop_last=drop_last,\n","            pin_memory=True,\n","            num_workers=num_workers\n","        )\n","\n","        return dataloader\n","    except Exception as e:\n","        print(e)\n","    \n","    "]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:50:05.895483Z","iopub.status.busy":"2025-07-17T19:50:05.894748Z","iopub.status.idle":"2025-07-17T19:50:06.149754Z","shell.execute_reply":"2025-07-17T19:50:06.149100Z","shell.execute_reply.started":"2025-07-17T19:50:05.895460Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of Train set:  750000\n","Size of Validation set:  250000\n"]}],"source":["# Info about dataset\n","train_ratio = 0.75\n","test_ratio = 0.25\n","total_dataset_size = len(book_corpus)\n","split_dataset = book_corpus.train_test_split(test_size=test_ratio, seed=47)\n","# splitting training set to train and validate\n","train_data = split_dataset['train']\n","validation_data = split_dataset['test']\n","print(\"Size of Train set: \", len(train_data))\n","print(\"Size of Validation set: \", len(validation_data))"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:50:09.285679Z","iopub.status.busy":"2025-07-17T19:50:09.285090Z","iopub.status.idle":"2025-07-17T19:51:19.585002Z","shell.execute_reply":"2025-07-17T19:51:19.584045Z","shell.execute_reply.started":"2025-07-17T19:50:09.285656Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Tokenizing and Chunking data...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a3e497944a04cf0b1fa466ee70bbe3f","version_major":2,"version_minor":0},"text/plain":["Tokenizing example:   0%|          | 0/750000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total Tokens Processed: 11864858\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e380d614be784031ba8483df582b6ad3","version_major":2,"version_minor":0},"text/plain":["Creating chunks:   0%|          | 0/23173 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Tokenizing and Chunking data...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c99d9db9d824268ae5eff2c53993c09","version_major":2,"version_minor":0},"text/plain":["Tokenizing example:   0%|          | 0/250000 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Total Tokens Processed: 3952477\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ae79ad9766ab43ab93ac9448da130837","version_major":2,"version_minor":0},"text/plain":["Creating chunks:   0%|          | 0/7719 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["train_loader = create_dataloader(\n","    train_data,\n","    batch_size=4,\n","    context_length=GPT2_CONFIG_124M[\"context_length\"],\n","    stride=GPT2_CONFIG_124M[\"context_length\"],\n","    drop_last=True,\n","    shuffle=True,\n","    num_workers=0,\n",")\n","\n","val_loader = create_dataloader(\n","    validation_data,\n","    batch_size=4,\n","    context_length=GPT2_CONFIG_124M[\"context_length\"],\n","    stride=GPT2_CONFIG_124M[\"context_length\"],\n","    drop_last=False,\n","    shuffle=False,\n","    num_workers=0,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Sanity checks before Training"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2025-07-16T19:41:57.399066Z","iopub.status.busy":"2025-07-16T19:41:57.398803Z","iopub.status.idle":"2025-07-16T19:41:57.541141Z","shell.execute_reply":"2025-07-16T19:41:57.540215Z","shell.execute_reply.started":"2025-07-16T19:41:57.399049Z"},"trusted":true},"outputs":[{"ename":"TypeError","evalue":"expected string or buffer","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3638225196.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msample_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_encoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtotal_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbook_corpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total_tokens : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tiktoken/core.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, allowed_special, disallowed_special)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisallowed_special\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mdisallowed_special\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisallowed_special\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0;34m:=\u001b[0m \u001b[0m_special_token_regex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisallowed_special\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m                 \u001b[0mraise_disallowed_special_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: expected string or buffer"]}],"source":["sample_tokenizer = tiktoken.get_encoding(\"gpt2\")\n","total_tokens = sample_tokenizer.encode(book_corpus['text'][:])\n","print(\"Total_tokens : \", total_tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Sanity Checks\n","if total_tokens * (train_ratio) < GPT2_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for training loader. Try to lower GPT2_CONFIG_124M['context_length']\")\n","\n","if total_tokens * (1 - train_ratio) < GPT2_CONFIG_124M[\"context_length\"]:\n","    print(\"Not enough tokens for validation set.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Examining Input and target matrices."]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2025-07-16T19:42:08.579525Z","iopub.status.busy":"2025-07-16T19:42:08.578792Z","iopub.status.idle":"2025-07-16T19:42:09.658184Z","shell.execute_reply":"2025-07-16T19:42:09.657298Z","shell.execute_reply.started":"2025-07-16T19:42:08.579494Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train loader: \n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","Validation loader: \n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([4, 512]) torch.Size([4, 512])\n","torch.Size([3, 512]) torch.Size([3, 512])\n"]}],"source":["print(\"Train loader: \")\n","for x, y in train_loader:\n","    print(x.shape, y.shape)\n","\n","print(\"Validation loader: \")\n","for x, y in val_loader:\n","    print(x.shape, y.shape)"]},{"cell_type":"markdown","metadata":{},"source":["## Loss function for model evaluation"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:51:19.586549Z","iopub.status.busy":"2025-07-17T19:51:19.586282Z","iopub.status.idle":"2025-07-17T19:51:19.593024Z","shell.execute_reply":"2025-07-17T19:51:19.592098Z","shell.execute_reply.started":"2025-07-17T19:51:19.586531Z"},"trusted":true},"outputs":[],"source":["def calculate_loss_batch(input_batch, target_batch, model, device):\n","    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n","    logits = model(input_batch)\n","    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n","    return loss\n","\n","def calculate_loss_loader(data_loader, model, device, num_batches=None):\n","    total_loss = 0.\n","    if len(data_loader) == 0:\n","        return float(\"nan\")\n","    elif num_batches is None:\n","        num_batches = len(data_loader)\n","    else:\n","        # reduce number of batches to match number of batches in the data loader\n","        num_batches = min(num_batches, len(data_loader))\n","\n","    for i, (input_batch, target_batch) in enumerate(data_loader):\n","        if i < num_batches:\n","            loss = calculate_loss_batch(input_batch, target_batch, model, device)\n","            total_loss += loss.item()\n","            \n","        else:\n","            break\n","    return total_loss / num_batches"]},{"cell_type":"markdown","metadata":{},"source":["## Model Evaluation"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:51:19.594073Z","iopub.status.busy":"2025-07-17T19:51:19.593835Z","iopub.status.idle":"2025-07-17T19:51:19.610138Z","shell.execute_reply":"2025-07-17T19:51:19.609392Z","shell.execute_reply.started":"2025-07-17T19:51:19.594048Z"},"trusted":true},"outputs":[],"source":["def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n","    # disable Dropout layer\n","    model.eval()\n","\n","    # Get predictions\n","    with torch.no_grad():\n","        train_loss = calculate_loss_loader(train_loader, model, device, num_batches=eval_iter)\n","        val_loss = calculate_loss_loader(val_loader, model, device, num_batches=eval_iter)\n","    model.train()\n","    return train_loss, val_loss"]},{"cell_type":"markdown","metadata":{},"source":["## Generate and Print Sample"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:51:19.611922Z","iopub.status.busy":"2025-07-17T19:51:19.611718Z","iopub.status.idle":"2025-07-17T19:51:19.629900Z","shell.execute_reply":"2025-07-17T19:51:19.629083Z","shell.execute_reply.started":"2025-07-17T19:51:19.611907Z"},"trusted":true},"outputs":[],"source":["def text_to_token_ids(text, tokenizer):\n","    encoded = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","    encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n","    return encoded_tensor\n","\n","def token_ids_to_text(token_ids, tokenizer):\n","    flat = token_ids.squeeze(0)\n","    return tokenizer.decode(flat.tolist())"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2025-07-16T23:02:30.803038Z","iopub.status.busy":"2025-07-16T23:02:30.802416Z","iopub.status.idle":"2025-07-16T23:02:30.807149Z","shell.execute_reply":"2025-07-16T23:02:30.806509Z","shell.execute_reply.started":"2025-07-16T23:02:30.803003Z"},"trusted":true},"outputs":[],"source":["def generate_and_print_sample(model, tokenizer, device, start_context):\n","    model.eval()\n","    context_size = model.pos_embeddings.weight.shape[0]\n","    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n","    with torch.no_grad():\n","        token_ids = generate_text(\n","            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n","        )\n","\n","    decoded_text = token_ids_to_text(token_ids, tokenizer)\n","    print(decoded_text.replace(\"\\n\", \" \"))\n","    model.train()"]},{"cell_type":"markdown","metadata":{},"source":["## Training Loop"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2025-07-16T23:02:33.440312Z","iopub.status.busy":"2025-07-16T23:02:33.439743Z","iopub.status.idle":"2025-07-16T23:02:33.445998Z","shell.execute_reply":"2025-07-16T23:02:33.445232Z","shell.execute_reply.started":"2025-07-16T23:02:33.440290Z"},"trusted":true},"outputs":[],"source":["def train_model_v1(model, train_loader, val_loader, optimizer,\n","          device, num_epochs, eval_freq, eval_iter,\n","          start_context, tokenizer):\n","\n","    # initlization of lists to track losses and token seen\n","    train_losses = val_losses = track_tokens_seen = []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main loop\n","    # For each epoch\n","    for epoch in range(num_epochs):\n","        model.train()\n","\n","        # For each batch in epoch\n","        for input_batch, target_batch in train_loader:\n","            # Reset loss gradients from previous batch\n","            optimizer.zero_grad()\n","\n","            # calculate loss\n","            loss = calculate_loss_batch(input_batch, target_batch, model, device)\n","\n","            # compute gradient loss\n","            loss.backward()\n","\n","            # update weights\n","            optimizer.step()\n","\n","            # update tokens seen at step\n","            tokens_seen += input_batch.numel()\n","\n","            # update epoch count\n","            global_step += 1\n","\n","            if global_step % eval_freq == 0:\n","                train_loss, val_loss = evaluate_model(\n","                    model, train_loader, val_loader, device, eval_iter\n","                )\n","\n","                train_losses.append(train_loss)\n","                val_losses.append(val_loss)\n","                track_tokens_seen.append(tokens_seen)\n","                print(f\"Epoch: {epoch+1} (Step {global_step:06d}): \"\n","                      f\"Train loss: {train_loss:.3f}, Val loss: {val_loss:.3f}\")\n","\n","        # generate output from tokens for visualization after each epoch\n","        generate_and_print_sample(\n","            model, tokenizer, device, start_context\n","        )\n","    return train_losses, val_losses, track_tokens_seen"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2025-07-16T23:02:41.540081Z","iopub.status.busy":"2025-07-16T23:02:41.539368Z","iopub.status.idle":"2025-07-16T23:02:41.546370Z","shell.execute_reply":"2025-07-16T23:02:41.545658Z","shell.execute_reply.started":"2025-07-16T23:02:41.540053Z"},"trusted":true},"outputs":[],"source":["def train_model_v2(model, train_loader, val_loader, optimizer,\n","          scheduler, device, num_epochs, eval_freq, eval_iter,\n","          start_context, tokenizer):\n","\n","    # initlization of lists to track losses and token seen\n","    train_losses = val_losses = track_tokens_seen = []\n","    tokens_seen, global_step = 0, -1\n","\n","    # Main loop\n","    # For each epoch\n","    for epoch in range(num_epochs):\n","        model.train()\n","\n","        total_train_loss = 0\n","\n","        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n","\n","        # For each batch in epoch\n","        for input_batch, target_batch in progress_bar:\n","            # Reset loss gradients from previous batch\n","            optimizer.zero_grad()\n","\n","            # calculate loss\n","            loss = calculate_loss_batch(input_batch, target_batch, model, device)\n","\n","            # compute gradient loss\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # update weights\n","            optimizer.step()\n","            scheduler.step()\n","\n","            total_train_loss += loss.item()\n","            # update tokens seen at step\n","            tokens_seen += input_batch.numel()\n","\n","            # update step count\n","            # global_step += 1\n","            progress_bar.set_postfix({\"loss\": f\"{loss.item():.3f}\"})\n","\n","        # Evaluation after each epoch\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = calculate_loss_loader(val_loader, model, device)\n","\n","        # avg training loss for each epoch\n","        avg_train_loss = total_train_loss / len(train_loader)\n","\n","        train_losses.append(avg_train_loss)\n","        val_losses.append(val_loss)\n","\n","        print(f\"Epoch {epoch+1:02d} | Avg Train Loss: {avg_train_loss:.3f} | Val loss: {val_loss:.3f}\")\n","\n","        # generate output from tokens for visualization after each epoch\n","        generate_and_print_sample(\n","            model, tokenizer, device, start_context\n","        )\n","    return train_losses, val_losses, track_tokens_seen"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:51:19.631621Z","iopub.status.busy":"2025-07-17T19:51:19.630926Z","iopub.status.idle":"2025-07-17T19:51:19.713026Z","shell.execute_reply":"2025-07-17T19:51:19.712262Z","shell.execute_reply.started":"2025-07-17T19:51:19.631602Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda device.\n"]}],"source":["if torch.cuda.is_available():\n","   device = torch.device(\"cuda\")\n","elif torch.backends.mps.is_available():\n","   device = torch.device(\"mps\")\n","else:\n","   device = torch.device(\"cpu\")\n","\n","print(f\"Using {device} device.\")"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2025-07-16T19:58:03.788321Z","iopub.status.busy":"2025-07-16T19:58:03.787849Z","iopub.status.idle":"2025-07-16T20:16:03.463859Z","shell.execute_reply":"2025-07-16T20:16:03.462821Z","shell.execute_reply.started":"2025-07-16T19:58:03.788297Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 1/5:   0%|          | 0/572 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 01 | Avg Train Loss: 2.919 | Val loss: 0.481\n","Every effort moves you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 2/5:   0%|          | 0/572 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 02 | Avg Train Loss: 0.271 | Val loss: 0.135\n","Every effort moves you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 3/5:   0%|          | 0/572 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 03 | Avg Train Loss: 0.072 | Val loss: 0.092\n","Every effort moves you big you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 4/5:   0%|          | 0/572 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 04 | Avg Train Loss: 0.032 | Val loss: 0.084\n","Every effort moves you moves you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 5/5:   0%|          | 0/572 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 05 | Avg Train Loss: 0.021 | Val loss: 0.084\n","Every effort moves you moves you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n","Training completed in 17.99 minutes.\n"]}],"source":["import time\n","start_time = time.time()\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n","\n","num_epochs = 5\n","\n","total_training_steps = num_epochs * len(train_loader)\n","warmup_steps = int(total_training_steps * 0.1)\n","\n","scheduler = get_cosine_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=warmup_steps,\n","    num_training_steps=total_training_steps,\n",")\n","\n","train_losses, val_losses, tokens_seen = train_model_v2(\n","    model, train_loader, val_loader, optimizer, scheduler, \n","    device, num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=\"he 'd seen the movie almost by mistake , considering he was a little young for the pg cartoon , but with older cousins \", tokenizer=tokenizer\n",")\n","\n","end_time = time.time()\n","execution_time = (end_time - start_time) / 60\n","print(f\"Training completed in {execution_time:.2f} minutes.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Temperature Scaling + Selecting Top-k Logits for Output Tokens "]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:51:19.714173Z","iopub.status.busy":"2025-07-17T19:51:19.713897Z","iopub.status.idle":"2025-07-17T19:51:19.728988Z","shell.execute_reply":"2025-07-17T19:51:19.728256Z","shell.execute_reply.started":"2025-07-17T19:51:19.714155Z"},"trusted":true},"outputs":[],"source":["def generate_text(model, idx, max_new_tokens, context_size, temperature=0.7, top_k=5, eos_id=None):\n","\n","    # 1. Get Logits.\n","    for _ in range(max_new_tokens):\n","        idx_cond = idx[:, -context_size:]\n","        with torch.no_grad():\n","            logits = model(idx_cond)\n","        logits = logits[:, -1, :]\n","\n","        # 2. Select Top_k elements\n","        if top_k is not None:\n","            top_logits, idx_numbers = torch.topk(logits, top_k)\n","            # 3. Set all other logits except top k to -inf\n","            min_val = top_logits[:, -1]\n","            logits = torch.where(logits < min_val, torch.tensor(float(\"-inf\")).to(logits.device), logits)\n","\n","        # 3. Scale by temperature value\n","        if temperature > 0.0:\n","            logits = logits / temperature\n","\n","            # 4. Apply softmax\n","            probs = torch.softmax(logits, dim=-1)\n","\n","            # 5. Sample from Multinomial distribution\n","            idx_next = torch.multinomial(probs, num_samples=1)\n","\n","        else:\n","            idx_next = torch.argmax(logits, dim=-1, keepdim=True)\n","\n","        if idx_next == eos_id:\n","            break\n","\n","        idx = torch.cat((idx, idx_next), dim=1)\n","\n","    return idx"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2025-07-17T19:51:19.729960Z","iopub.status.busy":"2025-07-17T19:51:19.729778Z","iopub.status.idle":"2025-07-17T19:51:19.774472Z","shell.execute_reply":"2025-07-17T19:51:19.773718Z","shell.execute_reply.started":"2025-07-17T19:51:19.729943Z"},"trusted":true},"outputs":[],"source":["def train_model_v3(model, train_loader, val_loader, optimizer,\n","          scheduler, device, num_epochs, eval_freq, eval_iter,\n","          start_context, tokenizer):\n","\n","    # initlization of lists to track losses and token seen\n","    train_losses = val_losses = track_tokens_seen = []\n","    tokens_seen = 0\n","\n","    # Main loop\n","    # For each epoch\n","    for epoch in range(num_epochs):\n","        model.train()\n","        total_train_loss = 0\n","        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n","\n","        # For each batch in epoch\n","        for input_batch, target_batch in progress_bar:\n","            # Reset loss gradients from previous batch\n","            optimizer.zero_grad()\n","\n","            # calculate loss\n","            loss = calculate_loss_batch(input_batch, target_batch, model, device)\n","\n","            # compute gradient loss\n","            loss.backward()\n","\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # update weights\n","            optimizer.step()\n","            scheduler.step()\n","\n","            total_train_loss += loss.item()\n","            # update tokens seen at step\n","            tokens_seen += input_batch.numel()\n","\n","            progress_bar.set_postfix({\"loss\": f\"{loss.item():.3f}\", \"lr\": f\"{scheduler.get_last_lr()[0]:.1e}\"})\n","\n","        # Evaluation after each epoch\n","        model.eval()\n","        with torch.no_grad():\n","            val_loss = calculate_loss_loader(val_loader, model, device)\n","\n","        # avg training loss for each epoch\n","        avg_train_loss = total_train_loss / len(train_loader)\n","\n","        train_losses.append(avg_train_loss)\n","        val_losses.append(val_loss)\n","\n","        print(f\"Epoch {epoch+1:02d} | Avg Train Loss: {avg_train_loss:.3f} | Val loss: {val_loss:.3f}\")\n","\n","        # generate output from tokens for visualization after each epoch\n","        start_ids = text_to_token_ids(start_context, tokenizer).to(device)\n","        context_size = model.pos_embeddings.weight.shape[0]\n","\n","        with torch.no_grad():\n","            output_ids = generate_text(\n","                model=model,\n","                idx=start_ids,\n","                max_new_tokens=50,\n","                context_size=context_size,\n","                top_k=50,\n","            )\n","\n","        generated_text = token_ids_to_text(output_ids, tokenizer)\n","        print(f\"Sample: {generated_text.replace(chr(10), ' ')}\")\n","    return train_losses, val_losses, track_tokens_seen"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"execution_failed":"2025-07-18T02:11:33.353Z","iopub.execute_input":"2025-07-17T20:02:05.210845Z","iopub.status.busy":"2025-07-17T20:02:05.209847Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 1/10:   0%|          | 0/5793 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 01 | Avg Train Loss: 2.133 | Val loss: 0.028\n","Sample: Every effort moves you  presenting wekeepers you we effort presentingkeepers invitations we we we you you you we you you you you we you you you you you you you you you you you you you you you you you you you you you you you you you we you you you\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 2/10:   0%|          | 0/5793 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 02 | Avg Train Loss: 0.029 | Val loss: 0.017\n","Sample: Every effort moves you  you person person person person effort you you you person person you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you you\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 3/10:   0%|          | 0/5793 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 03 | Avg Train Loss: 0.019 | Val loss: 0.015\n","Sample: Every effort moves you  preferring  moves reconciliation  reconciliation preferring preferring preferring preferring moves preferring preferring preferring preferring preferring preferring preferring enjoyment preferring  preferring preferring  preferring preferring strangers preferring  preferring preferring preferring preferring preferring strangers  preferring  preferring preferring enjoyment  preferring  reconciliation preferring   preferring preferring\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 4/10:   0%|          | 0/5793 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 04 | Avg Train Loss: 0.016 | Val loss: 0.014\n","Sample: Every effort moves you  ago reconciliation moves ago life effort ago effort ago lifelves ago ago life effort life ago ago life effort life life effort ago ago life ago life reconciliation ago life effort ago life ago life ago life life ago life life ago life ago life life life ago ago\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 5/10:   0%|          | 0/5793 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 05 | Avg Train Loss: 0.014 | Val loss: 0.014\n","Sample: Every effort moves you  hug moves moves that that effort that effort that that moves that that that effort that do that that that moves that effort that that moves that that that that that that that that that that that that that that that that that that that that really that that that\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 6/10:   0%|          | 0/5793 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 06 | Avg Train Loss: 0.012 | Val loss: 0.014\n","Sample: Every effort moves you  hug moves to hug hug effort hug graves lake hug moves hug to hug hug hug lake lake hug lake lake lake hug hug hug lake hug hug hug hug lake hug hug hug hug hug lake lake hug lake hug hug lake hug lake hug lake hug lake lake\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 7/10:   0%|          | 0/5793 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 07 | Avg Train Loss: 0.010 | Val loss: 0.014\n","Sample: Every effort moves you  hug moves anything hug hug effort hug anything anything toys anything hug anything hug hug anything anything anything hug anything anything anything anything anything anything anything anything anything anything anything anything anything hug anything hug hug anything anything hug anything anything anything anything anything anything hug anything anything anything anything\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 8/10:   0%|          | 0/5793 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 08 | Avg Train Loss: 0.008 | Val loss: 0.015\n","Sample: Every effort moves you  hug moves something something hug effort hug something somethingutch something something something something something something something something something something something something something effort something something things somethingk something something things even something something something somethingutch hug something something something something hug something hug something hug something something\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Epoch 9/10:   0%|          | 0/5793 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 09 | Avg Train Loss: 0.007 | Val loss: 0.015\n","Sample: Every effort moves you  than youttt effort than you than continued anything hugt than than than you than than thanading than than than than you than thant than than than than than than you than than than than than than thanading than than even than than than\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3948f15d340e4b08a487ce4de14beb82","version_major":2,"version_minor":0},"text/plain":["Epoch 10/10:   0%|          | 0/5793 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["import time\n","start_time = time.time()\n","\n","tokenizer = tiktoken.get_encoding(\"gpt2\")\n","model.to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n","\n","num_epochs = 10\n","\n","total_training_steps = num_epochs * len(train_loader)\n","warmup_steps = int(total_training_steps * 0.1)\n","\n","scheduler = get_cosine_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=warmup_steps,\n","    num_training_steps=total_training_steps,\n",")\n","\n","train_losses, val_losses, tokens_seen = train_model_v3(\n","    model, train_loader, val_loader, optimizer, scheduler, \n","    device, num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n","    start_context=\"Every effort moves you \", tokenizer=tokenizer\n",")\n","\n","end_time = time.time()\n","execution_time = (end_time - start_time) / 60\n","print(f\"Training completed in {execution_time:.2f} minutes.\")"]},{"cell_type":"markdown","metadata":{},"source":["## Saving Trained Model Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["torch.save({\n","    \"model_state_dict\": model.state_dict(),\n","    \"optimizer_state_dict\": optimizer.state_dict()\n","    },\n","    \"pre-trained_llm_and_optimizer.pth\")\n"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":7829030,"sourceId":12413689,"sourceType":"datasetVersion"}],"dockerImageVersionId":31089,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"}},"nbformat":4,"nbformat_minor":4}
